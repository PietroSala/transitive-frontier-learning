{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6edc0c9",
   "metadata": {},
   "source": [
    "# Air quality : Transitive frontier learnig Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb937099",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3570227345.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1133/3570227345.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    =2\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebda1413",
   "metadata": {},
   "source": [
    "Below there is an example of Transitive Frontier Learning implementation on the Air quality dataset.\n",
    "First we showed how the dataset was processed then how the analysis was carryed on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from math import log2  \n",
    "import pydotplus as ptp \n",
    "\n",
    "# helper_classes \n",
    "import import_ipynb\n",
    "import helper_general as my\n",
    "from ConstraintDefiner import ConstraintDefiner as Lark_CD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720ec58",
   "metadata": {},
   "source": [
    "## Loading + preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the correct rows and columns\n",
    "airQuality = pd.read_csv(\"./AirQualityUCI.csv\", sep=';').iloc[0:9356,:-2] \n",
    "display(airQuality.head(3))\n",
    "\n",
    "# Replacement the \",\" with \".\" in the scientific notations \n",
    "airQuality = airQuality.astype(str).applymap(lambda val: val.replace(',','.'))\n",
    "\n",
    "# Replacement the \"(\" and \")\" with \"_\" in the columns' name to prevent grammar's errors\n",
    "origNames = airQuality.columns.copy()\n",
    "airQuality.columns = [ colName.replace(\"(\",\"_\").replace(')','').replace('.','_') for colName in airQuality.columns.to_list()]\n",
    "\n",
    "# Add weekend\n",
    "airQuality['Date'] = (airQuality['Date'].map(lambda x: dt.datetime.strptime(x,'%d/%m/%Y')).dt.dayofweek >= 5).astype(int)\n",
    "airQuality = airQuality.rename(columns={'Date':'Weekend'})\n",
    "display(airQuality.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f8d05",
   "metadata": {},
   "source": [
    "## Mapping values, Encoder and Decoder\n",
    "Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e580d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values mapping into integer values\n",
    "encod , decod = {},{}\n",
    "for col in airQuality.columns.to_list():\n",
    "    \n",
    "    if col == \"Time\":\n",
    "        dec = airQuality[col].drop_duplicates().sort_values().reset_index(drop=True)\n",
    "    elif col == \"Weekend\":\n",
    "        dec = airQuality[col].drop_duplicates().sort_values().reset_index(drop=True)\n",
    "    else :\n",
    "        dec = airQuality[col].astype(float).drop_duplicates().sort_values() #float-> -200 == -200.0 \n",
    "        dec = dec.iloc[1:].reset_index(drop=True) # remove the -200.0 (None) => mapping only between not None values    \n",
    "    enc = dec.reset_index().set_index(col)['index']\n",
    "    \n",
    "    encod |= {col: dict(enc)| {-200.0: -1} } # In orther to have only integer values\n",
    "    decod |= {col: dict(dec)| {-1 : -200.0}}\n",
    "#my.myDisplay([encod['CO_GT'], decod['CO_GT']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f23c4",
   "metadata": {},
   "source": [
    "Encoding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AQ = pd.DataFrame() \n",
    "for col in airQuality.columns:\n",
    "    if col == \"Time\":\n",
    "         df_AQ[col] = airQuality[col].map(lambda val: encod[col][val])\n",
    "    else:\n",
    "        df_AQ[col] = airQuality[col].map(lambda val: encod[col][float(val)]) # cast in float -> '2' == '2.0' == 2.0\n",
    "df_AQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c2f46",
   "metadata": {},
   "source": [
    "Decoding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3851ac4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1133/3642577252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_AQ_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mdf_AQ_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_AQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_AQ_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_AQ_2 = pd.DataFrame() \n",
    "for col in airQuality.columns:\n",
    "         df_AQ_2[col] = df_AQ[col].map(lambda val: decod[col][val])\n",
    "df_AQ_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878ca84",
   "metadata": {},
   "source": [
    "## Decode Air Quality Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4de862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AQ_decodeRule(decod, df_X, df_Y):\n",
    "    def colDec(col, name):\n",
    "        return col.map(lambda v: decod[name][v])  \n",
    "    \n",
    "    X = df_X.T.apply(lambda col: colDec(col, col.name)).T\n",
    "    Y = df_Y.T.apply(lambda col: colDec(col, col.name)).T\n",
    "    \n",
    "    #--------------------------\n",
    "    # Pretty rule\n",
    "    X_pretty = X.index+'['+ X.L.astype(str)+', '+X.U.astype(str)+']'\n",
    "    Y_pretty = Y.index+'['+ Y.L.astype(str)+', '+Y.U.astype(str)+']'\n",
    "    s = f\"{', '.join(X_pretty.tolist())} --> {', '.join(Y_pretty.tolist())}\"\n",
    "    \n",
    "    return X,Y,s\n",
    "\n",
    "# dX,dY,ds = AQ_decodeRule(decod, X,Y) \n",
    "# my.myDisplay([dX,dY], axis=1)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87885fc9",
   "metadata": {},
   "source": [
    "## Support and Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e165f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supp_conf(df_Coded, df_X, df_Y, tempShow=False):\n",
    "    df_temp = pd.DataFrame()\n",
    "    for col in df_X.index:\n",
    "        df_temp[col] = (df_X['L'][col] <= df_Coded[col]) & (df_Coded[col] <= df_X['U'][col])\n",
    "    \n",
    "    for col in df_Y.index:\n",
    "        df_temp[col] = (df_Y['L'][col] <= df_Coded[col]) & (df_AQ[col] <= df_Y['U'][col])\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # Calcolo freq X e XY\n",
    "    freqX = df_temp[ df_temp[df_X.index].T.sum() == len(df_X)] # freq of X \n",
    "    freqXY = freqX[freqX[df_Y.index].T.sum() == len(df_Y)]     # freq of XY\n",
    "    \n",
    "    if tempShow : my.myDisplay([df_temp,freqX,freqXY],names=['Evaluation:','X:','XY:'], axis=1)\n",
    "    return {'supp':len(freqXY)/len(df_Coded) , 'conf':0 if len(freqX)==0 else len(freqXY)/len(freqX)}\n",
    "\n",
    "# X,Y,s = c.decodeRule(res)\n",
    "# suppConf = supp_conf(df_AQ, X, Y, True)\n",
    "# suppConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e1a30",
   "metadata": {},
   "source": [
    "## Transitive frontier learnig analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after preprocessing\n",
    "df_AQ.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbb305",
   "metadata": {},
   "source": [
    "Select the attributes to include in the analysed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8a51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_var = df_AQ.max().to_frame().reset_index()\n",
    "df_var['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3df3e",
   "metadata": {},
   "source": [
    "##  Create an instance of ConstraintDefiner (constrDef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7648b",
   "metadata": {},
   "source": [
    "This class was designed to create an interface between the user and the SAT-like core. \n",
    "It takes the variables and their maximum value to generate constraints which will guide the SAT-like search.\n",
    "\n",
    "Each variable was represented as an interval described whit a lower bound and an upper bound. Moreover, each interval value was used their their binary rappresetations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d99b6",
   "metadata": {},
   "source": [
    "The constraints implemented are: \n",
    "\n",
    "    init_constr(): Describes the rule structure and the intervals' rappresentation.\n",
    "    \n",
    "    NotSupported(res): Describes the constraint we want to apply if we find out a supported rule.\n",
    "    \n",
    "    NotConfident(res): Describes the constraint we want to apply if we find out a not confident rule.\n",
    "    \n",
    "    Confident(res):    Describes the constraint we want to apply if we find out a confident rule\n",
    "    \n",
    "    reject_solut(res): Describes the constraint we want to apply if we no longer want a rule.\n",
    "    \n",
    "Each implemented constraint return a string that should be inserted into a ConstraintDefiner's instance. \n",
    "We can insert a new constraint using the following method: .addConstr(constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222af805",
   "metadata": {},
   "source": [
    "Before starting we need to create a ConstraintDefiner's instance and insert rule structure's constraint. We can also set a seed for data reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Lark_CD(df_var['index'], df_var[0]) \n",
    "c.addConstr( c.init_constr() )  \n",
    "c.set_seed(2981050372)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bded5",
   "metadata": {},
   "source": [
    "### Heuristic implemented\n",
    "We can apply some heuristics to speed up the search by enabling the following method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ce153",
   "metadata": {},
   "source": [
    "###### set_resetTree(False): \n",
    "The ConstrDef instance keeps track of the values previously tested in previous searches, so as not to repeat paths already visited. if seleted to True, it will behave like a normal sat-solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d755e8f",
   "metadata": {},
   "source": [
    "###### set_priority(on_bits=opt)\n",
    "This heuristic priority set a priority between attributes. In this case, each attribute has the same probability of being assigned a value. \n",
    "\n",
    "The 'on_bits' set a priority over the value's bits, so we can consider the most significant bit first. There are three options implemented:\n",
    "\n",
    "    - none: There is no priority setted.\n",
    "    - 'Simple': In each interval values, will be evaluate the most significant bit first. \n",
    "    - 'Complete' First, will be evaluate the most significant bit among all the interval values, then the others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2a471",
   "metadata": {},
   "source": [
    "###### set_saturate_freeVars(opt):\n",
    "\n",
    "\n",
    "This heuristic set a priority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924983b7",
   "metadata": {},
   "source": [
    "###### set_pathExplorer(opt):\n",
    "This heuristic represent all the possible set a different method to explore the possible paths which will lead to a solutions.\n",
    "\n",
    "There options implemented are:\n",
    "    \n",
    "    random: Eac\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e58d91",
   "metadata": {},
   "source": [
    " we can set each heulistic as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b138850",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.set_seed(2981050372)  # we can set a seed to reproducibility\n",
    "c.set_pathExplorer('soft_max') #c.set_childrenChoice('min_power') #'random' or 'min_avgLenPath' or 'min_power' (soft_max)\n",
    "c.set_priority(on_bits='Complete') # none, 'Simple', 'Complete'\n",
    "c.set_saturate_freeVars(saturate='effective') # None, 'all', 'effective'\n",
    "c.set_resetTree(False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce299d",
   "metadata": {},
   "source": [
    "#### Here there is an instance of analysis setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide support and confidence and set the durations\n",
    "supp = 0.05\n",
    "conf = 0.8\n",
    "duration = '10m' # test durations (s:seconds, m:minutes, h:hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935ec07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i, allBool = 0, False\n",
    "sol, state_Sol, data_Solut= [], [], []\n",
    "\n",
    "#--------------------\n",
    "duration = pd.Timedelta(duration)\n",
    "startTime = dt.datetime.now() \n",
    "\n",
    "#---------------------------\n",
    "while (dt.datetime.now() - startTime ) < duration:  \n",
    "    start_run = dt.datetime.now()\n",
    "    nChar = sum([len(constr) for constr in c.constr])\n",
    "    print(f\"Run:{i+1}, #char:{nChar}, Started:{start_run.strftime('%m-%d %H:%M:%S')}, \", end='')\n",
    "    print(\"\")\n",
    "    #-----------------------\n",
    "    # SAT\n",
    "    res = c.Sat_Solver(test='speed') # only for test, else:  c.Sat_Solver()\n",
    "    end_run = dt.datetime.now()\n",
    "    \n",
    "    if res is None: break    # noSoluzione\n",
    "    if len(res)==0:  \n",
    "        allBool=True    # anly true and false -> ris={}\n",
    "        break\n",
    "       \n",
    "    #-----------------------\n",
    "    # Supp + Conf\n",
    "    X,Y,s = c.decodeRule(res) # X -> antecedent, Y-> consequent, L->lower bount, U->upper bound\n",
    "    suppConf = supp_conf(df_AQ, X, Y)\n",
    "    \n",
    "    sol = {'solut':s } | suppConf\n",
    "    if suppConf['supp'] < supp:\n",
    "        c.addConstr( c.NotSupported(res) )\n",
    "        sol |= {'state':'Not supported'}\n",
    "    \n",
    "    elif suppConf['conf'] < conf:\n",
    "        c.addConstr( c.NotConfident(res) )\n",
    "        sol |= {'state':'Not confident'}\n",
    "    \n",
    "    else:\n",
    "        c.addConstr( c.Confident(res) )\n",
    "        sol |= {'state':'Confident'}\n",
    "    \n",
    "    c.addConstr(  c.reject_solut(res) )\n",
    "    print(f\"runTime:{str(end_run - start_run)[:-7]}, TotTime:{str(end_run-startTime)[:-7]}, State:{sol['state']},\")\n",
    " \n",
    "    #-----------------------------\n",
    "    # Save sol\n",
    "    state_Sol.append(sol) \n",
    "    \n",
    "    X[['Side','idRule']], Y[['Side','idRule']] = ('X',i),('Y',i)\n",
    "    data_Solut.append([X,Y])\n",
    "    i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63382bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save state of the tested rules\n",
    "pd.DataFrame(state_Sol).to_csv('SCE_state.csv')\n",
    "pd.DataFrame(state_Sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the rule with range attribute\n",
    "pd.concat([pd.concat(d).reset_index().set_index('idRule') for d in data_Solut]).reset_index().to_csv('SCE_solutions.csv',index=False)\n",
    "pd.concat([pd.concat(d).reset_index().set_index('idRule') for d in data_Solut]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c57bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
