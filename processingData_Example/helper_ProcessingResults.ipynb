{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4b5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from helper.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydotplus as ptp\n",
    "\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "\n",
    "# my classes\n",
    "import import_ipynb\n",
    "import helper_general as my"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce4944",
   "metadata": {},
   "source": [
    "## Supp, Conf, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175137d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRule(df_X, df_Y):\n",
    "    df_X = df_X.apply(lambda row: f\"{row['V']}[{row['L']}-{row['U']}]\", axis=1)\n",
    "    df_Y = df_Y.apply(lambda row: f\"{row['V']}[{row['L']}-{row['U']}]\", axis=1)\n",
    "    return ', '.join(df_X.values)+' --> '+', '.join(df_Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45c447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_supp_conf(df_database, df_X, df_Y ):\n",
    "    # for each attribute, find the tuple that satisfied its interval\n",
    "    posX = df_X.apply( lambda row: (row.L <= df_database[row.V]) & ( df_database[row.V] <= row.U), axis=1)\n",
    "    posY = df_Y.apply( lambda row: (row.L <= df_database[row.V]) & ( df_database[row.V] <= row.U), axis=1)\n",
    "    #my.Display([posX, posY], names=['posX', 'posY'],axis=1)\n",
    "    \n",
    "    # for each tuple(columns) -> look if satisfies very attrubute\n",
    "    posX = posX.sum()==len(df_X) \n",
    "    posY = posY.sum()==len(df_Y)\n",
    "    posXY = posX & posY\n",
    "    #my.Display([posX, posY,posXY], names=['posX', 'posY', 'posXY'],axis=1)\n",
    "    \n",
    "    r = {'count_Ant':posX.sum(), 'sup_Antec': posX.sum()/len(df_database),\n",
    "         'count_Cons':posY.sum(), 'sup_Cons': posY.sum()/len(df_database),\n",
    "         'count_Rule':posXY.sum(), 'sup_Rule': posXY.sum()/len(df_database),\n",
    "         'conf_Rule':0 if posX.sum()==0 else posXY.sum()/posX.sum()}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477f2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_State(ruleInfo, supp, conf):\n",
    "    if ruleInfo['sup_Rule'] < supp:\n",
    "        return 'Not supported'\n",
    "    if ruleInfo['conf_Rule'] < conf:\n",
    "        return 'Not confident'\n",
    "    return 'Confident'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b83633",
   "metadata": {},
   "source": [
    "## Generalization Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140ae6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalizations(df_Rules): # rule_state\n",
    "    \n",
    "    # NB: Funziona per le regole scritte in formato: pd.DataFrame(columns={'idRule','V','L','U','Side'})\n",
    "    \n",
    "    # Preprocessing:-------------------------------------------\n",
    "    tempX = df_Rules[df_Rules['Side'] == 'X']\n",
    "    tempY = df_Rules[df_Rules['Side'] == 'Y']\n",
    "    df_Origin_lenX = tempX.groupby('idRule').agg({'V':'count'}).rename(columns={'V':'lenAnt_Orig'}).reset_index()\n",
    "    df_Origin_lenY = tempY.groupby('idRule').agg({'V':'count'}).rename(columns={'V':'lenCon_Orig'}).reset_index()\n",
    "    #my.Display([tempX, df_Origin_lenX, tempY, df_Origin_lenY],names=['tempX', 'df_Origin_lenX', 'tempY', 'df_Origin_lenY'], axis=1) \n",
    "\n",
    "    # Only Antecedent:----------------------------------------------\n",
    "    # Identifico attributi degli Antecedenti che generalizzano altri Antecedenti\n",
    "    tempX = pd.merge(tempX, tempX, on=['V','Side'], suffixes=('_G','_g'))  # Merge: G generalizza g (or g specializza G)\n",
    "    tempX = tempX[ tempX['idRule_G'] != tempX['idRule_g'] ] # rimuovo rules in join con se stesse\n",
    "    tempX = tempX[(tempX['L_G'] <= tempX['L_g']) & (tempX['U_g'] <= tempX['U_G'])] # generalizzo check: allargo Attributo degll'Antecedente\n",
    "    #tempX = tempX[(tempX['L_g'] <= tempX['L_G']) & (tempX['U_G'] <= tempX['U_g'])] # specializzo check: restringo Attributo degll'Antecedente\n",
    "\n",
    "    # Affinchè G generalizzi g: len(G[Att] intersecato g[Att]) == len(g[Att]): Since ALL g[ATT] devono essere presenti. OK if len(G[Att])>=len(g[Att])\n",
    "    df_lenAnt = tempX.groupby(['idRule_G','idRule_g'])['V'].count().reset_index().rename(columns={'V':'lenAnt_G&g'})\n",
    "    df_lenAnt = pd.merge(df_Origin_lenX, df_lenAnt, left_on=['idRule'], right_on=['idRule_G'])\n",
    "    df_Ant = df_lenAnt[ df_lenAnt['lenAnt_Orig']==df_lenAnt['lenAnt_G&g']]\n",
    "\n",
    "    # Only Conseguent:----------------------------------------------\n",
    "    # Identifico attributi dei Conseguenti che generalizzano altri Conseguenti\n",
    "    tempY = pd.merge(tempY, tempY, on=['V','Side'], suffixes=('_G','_g')) # Merge: G generalizza g\n",
    "    tempY = tempY[ tempY['idRule_G'] != tempY['idRule_g'] ] # rimuovo rule join con se stesse\n",
    "    tempY = tempY[(tempY['L_g'] <= tempY['L_G']) & (tempY['U_G'] <= tempY['U_g'])] # generalizzo check: restringo Attributo conseguente\n",
    "    #tempY = tempY[(tempY['L_G'] <= tempY['L_g']) & (tempY['U_g'] <= tempY['U_G'])] # specializzo check: allargo Attributo conseguente\n",
    "\n",
    "    # Affinchè G generalizzi g: len(G[Att] intersecato g[Att]) == len(A[Att]): Since ALL G[Att] devono essere presenti. OK if len(g[Att])>=len(G[Att])\n",
    "    df_lenCon = tempY.groupby(['idRule_G','idRule_g'])['V'].count().reset_index().rename(columns={'V':'lenCon_G&g'})\n",
    "    df_lenCon = pd.merge(df_Origin_lenY, df_lenCon, left_on=['idRule'], right_on=['idRule_g'])\n",
    "    df_Con =  df_lenCon[ df_lenCon['lenCon_Orig']==df_lenCon['lenCon_G&g']]\n",
    "    #my.Display([tempY, df_lenCon, df_Con], axis=1)\n",
    "\n",
    "    # Concrete Rules:---------------------------------------------------\n",
    "    # G generlizza g se esistone la coppia (id_G, id_g) sia in df_Ant che in df_Con\n",
    "    df_idRuleGeneral = pd.merge(df_Ant, df_Con, on=['idRule_G','idRule_g'])[['idRule_G','idRule_g']] \n",
    "    return df_idRuleGeneral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f9a78",
   "metadata": {},
   "source": [
    "## Tree Generalization Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28841f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFS_Closure( startNode, next_Nodes, dfAdj):\n",
    "    closureList = []\n",
    "    for u in next_Nodes:\n",
    "        if (not pd.isna(u)) and (dfAdj.at[u,'visited']==0): #  NON è già stato visitato \n",
    "            dfAdj.at[u,'visited'] = 1\n",
    "            closureList += [(startNode, u)] + DFS_Closure(startNode, dfAdj.at[u,'T'], dfAdj)\n",
    "    return closureList\n",
    "\n",
    "def transitive_closure(edgeList): # FUNZIA se NON ci sono cicli\n",
    "    dfEdges = pd.DataFrame(data=edgeList, columns=['S','T']) # Source, Target\n",
    "\n",
    "    # Find and add leaves + create dataframe Adj\n",
    "    leaves = set(dfEdges['T'])- set(dfEdges['S'])\n",
    "    dfEdges = pd.concat([dfEdges, pd.DataFrame(data=leaves, columns=['S'])])\n",
    "    dfAdj = dfEdges.groupby(['S']).agg({'T':list}).assign(visited = 0)\n",
    "\n",
    "    closureEdgeList = []\n",
    "    for v in dfAdj.index:\n",
    "        closureEdgeList += DFS_Closure(v, dfAdj.at[v,'T'], dfAdj)     \n",
    "        dfAdj['visited'] = 0\n",
    "\n",
    "    return pd.DataFrame(data=closureEdgeList, columns=['S','T'])  \n",
    "\n",
    "def pruneEdge(node, adj, edgeList_tranClosure ): # edgeList della\n",
    "    temp_adj = adj.copy()\n",
    "    df_tranClosure = pd.DataFrame(edgeList_tranClosure, columns=['S','T'])\n",
    "    for u in adj:\n",
    "        #print(f\"edge:{node}-{u}\")\n",
    "        V_target =  set(df_tranClosure[node == df_tranClosure['S']]['T']) # ciò che può partire ad node\n",
    "        U_sorgent = set(df_tranClosure[u == df_tranClosure['T']]['S'] )     # ciò che può arrivare ad u\n",
    "        inters = V_target.intersection(U_sorgent)\n",
    "        #my.Display([V_target, U_sorgent, inters],axis=1)\n",
    "        if len(inters)!=0:\n",
    "            temp_adj.remove(u)\n",
    "    return temp_adj    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4ee81",
   "metadata": {},
   "source": [
    "# Visualizzazione Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6237a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEdge(graph, node1, node2, color='black'):\n",
    "    n1 = ptp.Node( str(node1), shape='box', style='rounded') # questo caso, id is the same of the label\n",
    "    n1.set('label', str(node1) )\n",
    "    graph.add_node(n1)\n",
    "    \n",
    "    # check if node2 already exits -> if not, create now\n",
    "    n2 = graph.get_node(str(node2)) # return a list\n",
    "    if len(n2) == 0 : # Not exits -> empty list\n",
    "        n2 = ptp.Node( str(node2), shape='box', style='rounded') # questo caso, id is the same of the label\n",
    "        n2.set('label', str(node2) )\n",
    "        graph.add_node(n2)\n",
    "    else:\n",
    "        n2 = n2[0]# estraggo dalla lista\n",
    "    \n",
    "    # Create edge\n",
    "    edge = ptp.Edge(n1, n2)#, label='G')\n",
    "    edge.set(\"color\", color)\n",
    "    graph.add_edge(edge)\n",
    "    \n",
    "\n",
    "# gTEST = ptp.Dot()\n",
    "# adj_ruleTEST.apply(lambda adj: [addEdge(gTEST, adj.name, u) for u in adj['nodeList']], axis=1)\n",
    "# #gTEST.write_svg(f'adj_ruleTEST.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645b97c",
   "metadata": {},
   "source": [
    "# Load file for PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ddc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadReport_TEST(filename):\n",
    "    print(filename)\n",
    "    with open(filename) as f:\n",
    "        line = f.readlines()\n",
    "\n",
    "        start=0\n",
    "        for l in line:\n",
    "            if l[0:2]=='#=':\n",
    "                break\n",
    "            start += 1\n",
    "        rawData = line[start+1:]\n",
    "\n",
    "    #remove empty lines\n",
    "    try:\n",
    "        while(rawData.remove('\\n')):\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # remove last part of interrupt research\n",
    "    while rawData[-1][0:3] != 'run':\n",
    "        rawData.pop()\n",
    "\n",
    "    \n",
    "    # divide in run and subrun \n",
    "    data_Run_1 = []\n",
    "    data_Run_2 = []\n",
    "    data_SubRun = []\n",
    "    \n",
    "    idSubrun = -1\n",
    "    for l in rawData:\n",
    "        if l[0:3]=='Run':\n",
    "            data_Run_1.append( l.split(', ') )\n",
    "            idSubrun += 1\n",
    "        elif l[0:3]=='-->':\n",
    "            data_SubRun.append([idSubrun]+l.split(', '))\n",
    "        else:\n",
    "            data_Run_2.append(l.split(', '))\n",
    "          \n",
    "    # dataframe Runs :--------------------------------------------\n",
    "    data_Run_1 = pd.DataFrame(data_Run_1).drop([0,3], axis=1).rename(columns={1:'lenghtConstr', 2:'run_startTime'})\n",
    "    data_Run_2 = pd.DataFrame(data_Run_2).rename(columns={ 0:'run_time', 1:'run_totTime', 2:'state'})\n",
    "    \n",
    "    data =  pd.concat([data_Run_1, data_Run_2], axis=1) \n",
    "    data = data.reset_index().rename(columns={'index':'ID_Run'})\n",
    "    # refinements\n",
    "    data['lenghtConstr'] = data['lenghtConstr'].map(lambda x: x.split(':')[1:][0])\n",
    "    data['run_startTime'] = data['run_startTime'].map(lambda x: x.split('d:')[1:][0])\n",
    "    data['run_time'] = data['run_time'].map(lambda x: x.split('e:')[1:][0])\n",
    "    data['run_totTime'] = data['run_totTime'].map(lambda x: x.split('e:')[1:][0])\n",
    "    data['state'] = data['state'].map(lambda x: x.split(':')[1:][0][:-2])\n",
    "   \n",
    "    # time conversion\n",
    "    zero = datetime.strptime( \"00:00\", \"%H:%M\") \n",
    "    data['run_startTime'] = data['run_startTime'].map(lambda t: datetime.strptime(t,'%m-%d %H:%M:%S'))\n",
    "    data['run_time'] = data['run_time'].map(lambda t: datetime.strptime(t,'%H:%M:%S'))-zero\n",
    "    data['run_totTime'] = data['run_totTime'].map(lambda t: datetime.strptime(t,'%H:%M:%S'))#-zero più comodo come asseX\n",
    "    #display(data)\n",
    "    \n",
    "    # dataframe SubRuns :--------------------------------------------\n",
    "    data_SubRun =  pd.DataFrame(data_SubRun).drop([1,3,5], axis=1).rename(columns={0:'ID_Run', 2:'ID_SubRun', 4:'subRun_time'})\n",
    "    data_SubRun['ID_SubRun'] = data_SubRun['ID_SubRun'].map(lambda x: x.split(':')[1])\n",
    "    data_SubRun['subRun_time'] = data_SubRun['subRun_time'].map(lambda x: x.split('e:')[1:][0][:-1])\n",
    "    \n",
    "    # time conversion\n",
    "    data_SubRun['subRun_time'] = data_SubRun['subRun_time'].map(lambda t: datetime.strptime(t,'%H:%M:%S.%f'))-zero\n",
    "    data_SubRun['subRun_totTime'] = data_SubRun['subRun_time'].cumsum()+zero # non precisissimo, ma utile al momento\n",
    "    \n",
    "    return data, data_SubRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3fc9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadReport_up(filename):\n",
    "    print(filename)\n",
    "    with open(filename) as f:\n",
    "        line = f.readlines()\n",
    "\n",
    "        start=0\n",
    "        for l in line:\n",
    "            if l[0:2]=='#=':\n",
    "                break\n",
    "            start += 1\n",
    "        rawData = line[start+1:]\n",
    "\n",
    "    #remove empty lines\n",
    "    try:\n",
    "        while(rawData.remove('\\n')):\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # remove last part of interrupt research\n",
    "    while rawData[-1][0:3] != 'run':\n",
    "        rawData.pop()\n",
    " \n",
    "    # divide in run and subrun \n",
    "    data_Run_1 = []\n",
    "    data_Run_2 = []\n",
    "    count_UD = []\n",
    "    \n",
    "    for l in rawData:\n",
    "        if l[0:3]=='Run':\n",
    "            data_Run_1.append( l.split(', ') )\n",
    "        elif l[0:3]=='UD,':\n",
    "            count_UD.append( [len(l.split(','))] )\n",
    "        else:\n",
    "            data_Run_2.append(l.split(', '))\n",
    "          \n",
    "    # dataframe Runs :--------------------------------------------\n",
    "    data_Run_1 = pd.DataFrame(data_Run_1).drop([0,3], axis=1).rename(columns={1:'lenghtConstr', 2:'run_startTime'})\n",
    "    data_Run_2 = pd.DataFrame(data_Run_2).rename(columns={ 0:'run_time', 1:'run_totTime', 2:'state'})\n",
    "    count_UD = pd.DataFrame(count_UD, columns=['count_UD'])\n",
    "    \n",
    "    data =  pd.concat([data_Run_1, count_UD, data_Run_2], axis=1) \n",
    "    data = data.reset_index().rename(columns={'index':'ID_Run'})\n",
    "\n",
    "    # refinements\n",
    "    data['lenghtConstr'] = data['lenghtConstr'].map(lambda x: x.split(':')[1:][0])\n",
    "    data['run_startTime'] = data['run_startTime'].map(lambda x: x.split('d:')[1:][0])\n",
    "    data['run_time'] = data['run_time'].map(lambda x: x.split('e:')[1:][0])\n",
    "    data['run_totTime'] = data['run_totTime'].map(lambda x: x.split('e:')[1:][0])\n",
    "    data['state'] = data['state'].map(lambda x: x.split(':')[1:][0][:-2])\n",
    "   \n",
    "    # time conversion\n",
    "    zero = datetime.strptime( \"00:00\", \"%H:%M\") \n",
    "    data['run_startTime'] = data['run_startTime'].map(lambda t: datetime.strptime(t,'%m-%d %H:%M:%S'))\n",
    "    data['run_time'] = data['run_time'].map(lambda t: datetime.strptime(t,'%H:%M:%S'))-zero\n",
    "    data['run_totTime'] = data['run_totTime'].map(lambda t: datetime.strptime(t,'%H:%M:%S'))#-zero più comodo come asseX\n",
    "    #display(data)\n",
    "    \n",
    "    # dataframe SubRuns :--------------------------------------------\n",
    "   \n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
